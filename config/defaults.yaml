# NBA True Strength â€” runtime config
# Source of truth: .cursor/plans/Plan.md

# Season boundaries (hard-coded to avoid play-in ambiguity)
seasons:
  "2014-15": { start: "2014-10-01", end: "2015-04-15" }  # prior season baseline only
  "2015-16": { start: "2015-10-01", end: "2016-04-15" }
  "2016-17": { start: "2016-10-01", end: "2017-04-15" }
  "2017-18": { start: "2017-10-01", end: "2018-04-15" }
  "2018-19": { start: "2018-10-01", end: "2019-04-15" }
  "2019-20": { start: "2019-10-01", end: "2020-04-15" }
  "2020-21": { start: "2020-12-01", end: "2021-05-15" }
  "2021-22": { start: "2021-10-01", end: "2022-04-15" }
  "2022-23": { start: "2022-10-01", end: "2023-04-15" }
  "2023-24": { start: "2023-10-01", end: "2024-04-15" }
  "2024-25": { start: "2024-10-01", end: "2025-04-15" }
  "2025-26": { start: "2025-10-01", end: "2026-04-15" }

paths:
  # Canonical DB with playoff data (playoff_games, playoff_team_game_logs) for sweeps and eval.
  # Use this same absolute path in every worktree so --objective playoff_spearman gets real metrics.
  # Override from any worktree with env: NBA_DB_PATH=/path/to/data/processed/nba_build.duckdb
  db: "C:/Users/tmaku/OneDrive/Documents/GSU/Advanced Machine Learning/NBA Playoff Strentgh Project/data/processed/nba_build.duckdb"
  raw: "data/raw"
  processed: "data/processed"
  outputs: "outputs3"
  # When set, script 3 caches built batches (lists, oof_batches, all_batches) keyed by config+DB.
  # Reuse on cache hit to save ~1-3 min per sweep trial. null = use data/processed/batch_cache.
  batch_cache: null

# Skip rebuilding DB if file already exists (saves existing DB; set false to force rebuild from raw)
build_db:
  skip_if_exists: true

repro:
  seed: 42

model_a:
  stat_dim: 21  # 18 (L10+L30 incl ts_pct, usage) + 2 (on_court_pm_approx L10/L30) + 1 (pct_min_returning)
  num_embeddings: 500
  embedding_dim: 32
  encoder_hidden: [128, 64]
  attention_heads: 4
  dropout: 0.2
  learning_rate: 0.0001
  grad_clip_max_norm: 5.0  # looser than 1.0 to avoid zero effective gradients when norm is large
  epochs: 28
  early_stopping_patience: 0
  early_stopping_min_delta: 0.0
  early_stopping_val_frac: 0.25
  minutes_bias_weight: 0.3
  minutes_sum_min: 1.0e-6
  attention_debug: false  # set true when debugging attention collapse; false for faster training
  # When raw attention collapses (nearly zero on valid positions): "minutes" or "uniform"
  attention_fallback_strategy: "minutes"
  # Automatic Mixed Precision (AMP) for faster training on CUDA; loss stays float32. Set false if numerical issues.
  use_amp: true

model_b:
  xgb:
    n_estimators: 250
    max_depth: 4
    learning_rate: 0.08
    early_stopping_rounds: 20
  rf:
    n_estimators: 200
    max_depth: 12
    min_samples_leaf: 5

training:
  n_folds: 5
  rolling_windows: [10, 30]
  roster_size: 15
  target_rank: "playoffs"  # or "standings" to train meta on standings-to-date
  listmle_target: "playoff_outcome"  # "playoff_outcome"=EOS playoff result (champion=1); "final_rank"=EOS standings; "standings"=win-rate to date
  # 75/25 train-test split (script 3 writes split_info.json; 4 and 6 read it)
  train_seasons: ["2015-16", "2016-17", "2017-18", "2018-19", "2019-20", "2020-21", "2021-22", "2022-23"]
  test_seasons: ["2023-24", "2024-25"]
  validation_seasons: ["2023-24"]
  holdout_seasons: ["2024-25"]
  train_test_cutoff: null   # optional date cutoff alternative, e.g. "2023-04-15"
  train_frac: 0.75          # fallback if seasons/cutoff not provided
  # Model A (script 3): subsample lists for feasible runtime
  max_lists_oof: 30        # max lists for OOF; subsampled when there are more
  max_final_batches: 50    # max lists for final Model A training (script 3)
  # build_lists (src/training/build_lists.py) subsamples dates to 200 when there are more
  # If true, use per-season walk-forward instead of pooled OOF (Model A only)
  walk_forward: false
  # Prior season baseline: fill zero player stats with prior season averages
  use_prior_season_baseline: true
  prior_season_lookback_days: 365

output:
  true_strength_scale: "percentile"  # or softmax, platt
  odds_temperature: 0.5  # softmax temperature for championship odds (lower = sharper)
  championship_odds_method: "softmax"  # or "monte_carlo"
  ig_inference_top_k: 1  # per conference; set 0 to disable IG in predictions.json
  ig_inference_steps: 50

# Elo with cold start (mandatory per plan; retrain Model B when enabled)
elo:
  cold_start_games: 10
  regression_to_mean: 0.25
  enabled: true

# Massey ratings (mandatory per plan; solve Mr = p per season)
massey:
  enabled: true

# SOS/SRS from data/raw/Team_Records.csv (Phase II data)
sos_srs:
  data_path: "data/raw/Team_Records.csv"
  enabled: false

# Team rolling (eFG_L10, DefRtg_L10, won_prev_game)
team_rolling:
  enabled: false

# Injury adjustments (ProjAvailableRating)
injury:
  data_path: "data/raw/injury_reports"
  minutes_heuristic: "proportional"
  enabled: false  # set true when injury JSON available

# Motivation (tanking/seeding)
motivation:
  late_season_games: 15
  playoff_wins_threshold: 42
  enabled: true

# RAPTOR (FiveThirtyEight) player impact metrics
raptor:
  data_path: "docs/modern_RAPTOR_by_team.csv"
  enabled: true

# Monte Carlo championship simulation
monte_carlo:
  hca: 100.0
  rating_scale: 400.0
  n_seasons: 10000
  n_series: 1000

logging:
  roster_debug: false
  playoff_debug: false

# Hyperparameter sweep (scripts/sweep_hparams.py)
# Smaller default grid for feasible full runs; use --phase phase1_xgb or phase2_rf for phased grids, or expand lists for full grid.
sweep:
  model_a_epochs: [16, 28]
  rolling_windows: [[10, 30]]
  model_b:
    max_depth: [3, 4]
    learning_rate: [0.08]
    n_estimators_xgb: [250, 300]
    n_estimators_rf: [200]
    subsample: [0.8]
    colsample_bytree: [0.7]
    min_samples_leaf: [5]
  include_clone_classifier: false  # set true to run 4c after eval; sweep runs faster without

# null = auto-increment run (run_002, run_003, ... from existing run_* dirs); set e.g. "run_001" to fix
inference:
  run_id: run_023
  # When outputs dir has no run_* subdirs, use this as first run (e.g. 19 -> run_019)
  run_id_base: 23
  # If true, also run inference on last train date and write run_id/train_predictions.json
  also_train_predictions: false
  # If true, inference must use eos_final_rank (playoff-based EOS). Fails if DB has no playoff data (sweep matches baseline).
  require_eos_final_rank: false